{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd892d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO model will use 11 engineered features.\n",
      "Training stops (samples): [100, 1000, 1500]\n",
      "Evaluation stops (trials/SNR): [5000]\n",
      "Models to evaluate: ['PPO-Engineered', 'LGBM', 'MSC', 'TFG']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Experiment Progress:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STARTING STAGE: 100 samples\n",
      "================================================================================\n",
      "Loading 100 samples for training...\n",
      "Loading up to 1 trials per SNR to gather enough samples.\n",
      "Calculating engineered features for PPO model...\n",
      "Data loaded. Starting model training...\n",
      "Training LGBM...\n",
      "Training PPO-Engineered with statistical reward...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexa\\OneDrive\\Área de Trabalho\\ord_combination_icassp2026\\.venv\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:155: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 100`, after every 1 untruncated mini-batches, there will be a truncated mini-batch of size 36\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=100 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Statistical Detectors (ORD)...\n",
      "\n",
      "--- Starting Evaluation for models trained with 100 samples ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Total Experiment Progress:   0%|          | 0/3 [04:46<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# CODE A (ADAPTED & OPTIMIZED with New Rewards and Experimental Validation)\n",
    "\"\"\"\n",
    "Full script for large-scale, iterative training and performance evaluation of detectors.\n",
    "\n",
    "V6 - STATISTICAL REWARD & EXPERIMENTAL VALIDATION\n",
    "This version introduces a more sophisticated reward mechanism for the PPO agent,\n",
    "basing the reward on TPR, FPR, and FPR variance over a moving window. It also adds\n",
    "a second phase for experimental validation, evaluating the models' performance\n",
    "before and after retraining on out-of-distribution experimental data.\n",
    "\"\"\"\n",
    "\n",
    "# %% 1. Imports and Configuration\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import lightgbm as lgb\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "from collections import deque\n",
    "\n",
    "# Ignore specific warnings to keep output clean\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"X does not have valid feature names\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Starting from version 2.2.1\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "# ##############################################################################\n",
    "# SECTION 1: USER-DEFINED PARAMETERS & CONFIGURATION\n",
    "# ##############################################################################\n",
    "\n",
    "# --- Experiment Control Parameters ---\n",
    "TRAINING_SAMPLE_STOPS = [100, 1000, 1500]\n",
    "EVAL_TRIAL_STOPS = [5000]\n",
    "\n",
    "# --- Model Training Parameters ---\n",
    "MODELS_TO_TRAIN_AND_EVAL = ['PPO-Engineered', 'LGBM', 'MSC', 'TFG']\n",
    "RL_TRAINING_EPOCHS = 10\n",
    "LGBM_N_ESTIMATORS = 100\n",
    "REWARD_WINDOW_SIZE = 18  # Window for calculating TPR/FPR based reward\n",
    "\n",
    "# --- Data and Path Configuration ---\n",
    "HDF5_TRAIN_FILEPATH = '../data/train_v4.hdf5'\n",
    "HDF5_EVAL_FILEPATH = '../data/train_v4.hdf5'\n",
    "EXPERIMENTAL_FEATURES_FILE = '../data/experimental_features.hdf5' # For Phase 2\n",
    "MODELS_DIR = './models' # For saving models for Phase 2\n",
    "\n",
    "# --- Signal and Feature Configuration ---\n",
    "NUM_SIGNAL_FREQS = 8\n",
    "ALL_SNRS = [-30.0, -27.5, -25.0, -22.5, -20.0, -17.5, -15.0, -12.5, -10.0, -7.5, -5.0]\n",
    "RAW_FEATURE_NAMES = ['MSC', 'CSM', 'TFG', 'TFL', 'SNR_meas', 'MAG_freq', 'PHI_freq']\n",
    "MAX_EVAL_TRIALS_PER_SNR = max(EVAL_TRIAL_STOPS)\n",
    "\n",
    "# ##############################################################################\n",
    "# FEATURE ENGINEERING CONFIGURATION\n",
    "# ##############################################################################\n",
    "FEATURE_CONFIG = [\n",
    "    # --- Simple Features (current value) ---\n",
    "    ('MSC', 'simple', {}),\n",
    "    ('CSM', 'simple', {}),\n",
    "    ('TFL', 'simple', {}),\n",
    "    ('SNR_meas', 'simple', {}),\n",
    "    ('MAG_freq', 'simple', {}),\n",
    "\n",
    "    # --- Moving Average Features ---\n",
    "    ('MSC', 'moving_mean', {'window_size': 18}),\n",
    "    ('CSM', 'moving_mean', {'window_size': 18}),\n",
    "    ('TFL', 'moving_mean', {'window_size': 18}),\n",
    "\n",
    "    # --- Cumulative Sum Features ---\n",
    "    ('MSC', 'cumsum', {}),\n",
    "    ('CSM', 'cumsum', {}),\n",
    "\n",
    "    # --- Time-based Feature ---\n",
    "    ('timestamp', 'simple', {})\n",
    "]\n",
    "print(f\"PPO model will use {len(FEATURE_CONFIG)} engineered features.\")\n",
    "# ##############################################################################\n",
    "\n",
    "print(f\"Training stops (samples): {TRAINING_SAMPLE_STOPS}\")\n",
    "print(f\"Evaluation stops (trials/SNR): {EVAL_TRIAL_STOPS}\")\n",
    "print(f\"Models to evaluate: {MODELS_TO_TRAIN_AND_EVAL}\")\n",
    "\n",
    "\n",
    "# ##############################################################################\n",
    "# SECTION 2: DATA LOADING AND PREPARATION FUNCTIONS (Unchanged)\n",
    "# ##############################################################################\n",
    "\n",
    "def my_norm(val, min_val=0, max_val=3):\n",
    "    \"\"\"Normalizes a value.\"\"\"\n",
    "    return (val - min_val) / (max_val - min_val) if max_val != min_val else 0.0\n",
    "\n",
    "def generate_features_from_config_vectorized(data_4d, feature_names, config):\n",
    "    \"\"\"\n",
    "    Generates engineered features using vectorized NumPy operations for high performance.\n",
    "    Shape: (trials, windows, freqs, features) -> (trials, windows, freqs, engineered_features)\n",
    "    \"\"\"\n",
    "    num_trials, num_windows, num_freqs, _ = data_4d.shape\n",
    "    feature_indices = {name: i for i, name in enumerate(feature_names)}\n",
    "    \n",
    "    engineered_features_list = []\n",
    "\n",
    "    for feature, transform, params in config:\n",
    "        if feature == 'timestamp':\n",
    "            ts = (np.arange(1, num_windows + 1) / num_windows).astype(np.float32)\n",
    "            engineered_feature = np.tile(ts.reshape(1, -1, 1, 1), (num_trials, 1, num_freqs, 1))\n",
    "            engineered_features_list.append(engineered_feature)\n",
    "            continue\n",
    "            \n",
    "        feature_idx = feature_indices[feature]\n",
    "        data_slice = data_4d[:, :, :, feature_idx]\n",
    "\n",
    "        if transform == 'simple':\n",
    "            engineered_feature = data_slice\n",
    "            \n",
    "        elif transform == 'cumsum':\n",
    "            engineered_feature = np.cumsum(data_slice, axis=1)\n",
    "\n",
    "        elif transform == 'cummean':\n",
    "            cumulative_sum = np.cumsum(data_slice, axis=1)\n",
    "            divisors = np.arange(1, num_windows + 1).reshape(1, -1, 1)\n",
    "            engineered_feature = cumulative_sum / divisors\n",
    "\n",
    "        elif transform == 'moving_mean':\n",
    "            window_size = params.get('window_size', 10)\n",
    "            reshaped_data = data_slice.transpose(0, 2, 1).reshape(-1, num_windows)\n",
    "            df = pd.DataFrame(reshaped_data.T)\n",
    "            rolling_mean_df = df.rolling(window=window_size, min_periods=1).mean()\n",
    "            rolling_mean_T = rolling_mean_df.to_numpy().T\n",
    "            engineered_feature = rolling_mean_T.reshape(num_trials, num_freqs, num_windows).transpose(0, 2, 1)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown transformation: {transform}\")\n",
    "        \n",
    "        engineered_features_list.append(my_norm(engineered_feature[..., np.newaxis]))\n",
    "\n",
    "    return np.concatenate(engineered_features_list, axis=-1, dtype=np.float32)\n",
    "\n",
    "def load_training_data_for_stage(filepath, snr_levels, total_samples_needed, num_signal_freqs):\n",
    "    \"\"\"\n",
    "    Loads and prepares a specific number of training samples from 4D HDF5 data.\n",
    "    Generates raw features for standard models and engineered features for PPO.\n",
    "    \"\"\"\n",
    "    X_all_snrs_4d = []\n",
    "    \n",
    "    with h5py.File(filepath, 'r') as f:\n",
    "        first_key = f\"snr_{snr_levels[0]:.1f}\"\n",
    "        if first_key not in f:\n",
    "                raise ValueError(f\"Initial SNR key {first_key} not found in HDF5 file.\")\n",
    "        _, windows_per_trial, freqs_per_trial, _ = f[first_key]['metrics'].shape\n",
    "        samples_per_trial = windows_per_trial * freqs_per_trial\n",
    "\n",
    "    samples_per_snr = math.ceil(total_samples_needed / len(snr_levels))\n",
    "    trials_per_snr = math.ceil(samples_per_snr / samples_per_trial)\n",
    "    \n",
    "    print(f\"Loading up to {trials_per_snr} trials per SNR to gather enough samples.\")\n",
    "    \n",
    "    with h5py.File(filepath, 'r') as f:\n",
    "        for snr in snr_levels:\n",
    "            key = f\"snr_{snr:.1f}\"\n",
    "            if key in f:\n",
    "                metrics_data_4d = f[key]['metrics'][:trials_per_snr, ...]\n",
    "                X_all_snrs_4d.append(metrics_data_4d)\n",
    "            else:\n",
    "                print(f\"Warning: Training SNR {snr} not found in {filepath}\")\n",
    "\n",
    "    if not X_all_snrs_4d:\n",
    "        return None\n",
    "\n",
    "    X_full_4d = np.vstack(X_all_snrs_4d)\n",
    "    \n",
    "    num_trials, num_windows, num_freqs, num_raw_features = X_full_4d.shape\n",
    "\n",
    "    freq_labels = np.zeros(num_freqs)\n",
    "    freq_labels[:num_signal_freqs] = 1\n",
    "    y_labels_4d = np.tile(freq_labels, (num_trials, num_windows, 1))\n",
    "\n",
    "    X_flat = X_full_4d.reshape(-1, num_raw_features)\n",
    "    y_flat = y_labels_4d.flatten()\n",
    "\n",
    "    p = np.random.permutation(len(X_flat))\n",
    "    X_flat_shuffled = X_flat[p]\n",
    "    y_flat_shuffled = y_flat[p]\n",
    "    \n",
    "    samples_to_take = min(total_samples_needed, len(X_flat_shuffled))\n",
    "    X_train_flat = X_flat_shuffled[:samples_to_take]\n",
    "    y_train_flat = y_flat_shuffled[:samples_to_take]\n",
    "    \n",
    "    print(\"Calculating engineered features for PPO model...\")\n",
    "    X_engineered_4d = generate_features_from_config_vectorized(X_full_4d, RAW_FEATURE_NAMES, FEATURE_CONFIG)\n",
    "    _, _, _, num_engineered_features = X_engineered_4d.shape\n",
    "    X_engineered_flat = X_engineered_4d.reshape(-1, num_engineered_features)\n",
    "    \n",
    "    X_train_engineered = X_engineered_flat[p][:samples_to_take]\n",
    "\n",
    "    return {\n",
    "        'flat': (X_train_flat, y_train_flat),\n",
    "        'engineered': (X_train_engineered, y_train_flat)\n",
    "    }\n",
    "\n",
    "def load_evaluation_data_for_snr(filepath, snr, trials_per_snr, num_signal_freqs):\n",
    "    try:\n",
    "        with h5py.File(filepath, 'r') as f:\n",
    "            key = f\"snr_{snr:.1f}\"\n",
    "            if key not in f:\n",
    "                return np.array([]), np.array([])\n",
    "            \n",
    "            metrics_data_4d = f[key]['metrics'][:trials_per_snr, ...]\n",
    "\n",
    "        signal_data_4d = metrics_data_4d[:, :, :num_signal_freqs, :]\n",
    "        noise_data_4d = metrics_data_4d[:, :, num_signal_freqs:, :]\n",
    "        \n",
    "        return signal_data_4d, noise_data_4d\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading evaluation data for SNR {snr}: {e}\")\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "\n",
    "# ##############################################################################\n",
    "# SECTION 3: MODELS, ENVIRONMENT, AND HELPER FUNCTIONS (REWARD ENV MODIFIED)\n",
    "# ##############################################################################\n",
    "\n",
    "class SignalDetectionEnvWithStats(gym.Env):\n",
    "    \"\"\"\n",
    "    A Gym environment for signal detection that provides rewards based on\n",
    "    TPR, FPR, and FPR variance calculated over a moving window of recent decisions.\n",
    "    \"\"\"\n",
    "    def __init__(self, features, labels, window_size=18):\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.window_size = window_size\n",
    "        self.history = deque(maxlen=self.window_size)\n",
    "        self.current_step = 0\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(features.shape[1],), dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        predicted_label = 1 if action[0] > 0 else 0\n",
    "        true_label = self.labels[self.current_step]\n",
    "        \n",
    "        self.history.append((predicted_label, true_label))\n",
    "        \n",
    "        # Calculate reward based on the history\n",
    "        if len(self.history) < self.window_size:\n",
    "            # Provide a simple, immediate reward until the window is full\n",
    "            reward = 1.0 if predicted_label == true_label else -1.0\n",
    "        else:\n",
    "            hist_preds = np.array([h[0] for h in self.history])\n",
    "            hist_labels = np.array([h[1] for h in self.history])\n",
    "            \n",
    "            # True Positives, False Positives\n",
    "            tp = np.sum((hist_preds == 1) & (hist_labels == 1))\n",
    "            fp = np.sum((hist_preds == 1) & (hist_labels == 0))\n",
    "            \n",
    "            # Total actual positives and negatives\n",
    "            actual_pos = np.sum(hist_labels == 1)\n",
    "            actual_neg = np.sum(hist_labels == 0)\n",
    "            \n",
    "            tpr = tp / actual_pos if actual_pos > 0 else 0\n",
    "            fpr = fp / actual_neg if actual_neg > 0 else 0\n",
    "            \n",
    "            # Variance of FPR (variance of predictions on negative samples)\n",
    "            fpr_decisions = hist_preds[hist_labels == 0]\n",
    "            fpr_var = np.var(fpr_decisions) if len(fpr_decisions) > 1 else 0\n",
    "            \n",
    "            reward = tpr - fpr - fpr_var\n",
    "\n",
    "        self.current_step += 1\n",
    "        terminated = self.current_step >= len(self.features)\n",
    "        next_obs = self.features[self.current_step] if not terminated else np.zeros(self.observation_space.shape)\n",
    "        \n",
    "        return next_obs, reward, terminated, False, {}\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_step = 0\n",
    "        self.history.clear()\n",
    "        return self.features[0], {}\n",
    "\n",
    "class ORDTester:\n",
    "    def __init__(self, desired_alpha=0.05, feature_names=RAW_FEATURE_NAMES):\n",
    "        self.desired_alpha = desired_alpha\n",
    "        self.feature_names = feature_names\n",
    "        self.thresholds = {}\n",
    "    def train(self, noise_features):\n",
    "        for i, key in enumerate(self.feature_names):\n",
    "            if noise_features.shape[0] > 0:\n",
    "                self.thresholds[key] = np.percentile(noise_features[:, i], (1 - self.desired_alpha) * 100)\n",
    "    def predict(self, features):\n",
    "        decisions = {}\n",
    "        for i, name in enumerate(self.feature_names):\n",
    "            if name in self.thresholds:\n",
    "                decisions[name] = (features[:, i] > self.thresholds[name]).astype(int)\n",
    "            else:\n",
    "                decisions[name] = np.zeros(features.shape[0], dtype=int)\n",
    "        return decisions\n",
    "\n",
    "def predict_lgbm(model, features_flat): return model.predict(features_flat)\n",
    "\n",
    "def predict_ppo(model, features_flat):\n",
    "    preds, _ = model.predict(features_flat, deterministic=True)\n",
    "    preds[np.isnan(preds)] = 0\n",
    "    return (preds > 0).astype(int)\n",
    "\n",
    "def predict_statistical(model, features_flat, detector_name):\n",
    "    decisions = model.predict(features_flat)\n",
    "    return decisions.get(detector_name, np.zeros(features_flat.shape[0], dtype=int))\n",
    "\n",
    "def calculate_statistics(predictions_per_trial):\n",
    "    per_trial_metric = np.mean(predictions_per_trial, axis=1)\n",
    "    return {\n",
    "        'mean': np.mean(per_trial_metric),\n",
    "        'variance': np.var(per_trial_metric),\n",
    "        'p1': np.percentile(per_trial_metric, 1),\n",
    "        'p99': np.percentile(per_trial_metric, 99)\n",
    "    }\n",
    "\n",
    "# ##############################################################################\n",
    "# SECTION 4: MAIN EXPERIMENT AND ANALYSIS LOOP\n",
    "# ##############################################################################\n",
    "def run_full_experiment():\n",
    "    \"\"\"Main function to run the entire training and evaluation pipeline.\"\"\"\n",
    "    all_results_over_time = []\n",
    "    policy_kwargs = dict(net_arch=dict(pi=[64, 64], vf=[64, 64]))\n",
    "    CHECKPOINT_TRAINING_SIZE = max(TRAINING_SAMPLE_STOPS)\n",
    "\n",
    "    for training_size in tqdm(TRAINING_SAMPLE_STOPS, desc=\"Total Experiment Progress\"):\n",
    "        print(f\"\\n{'='*80}\\nSTARTING STAGE: {training_size:,} samples\\n{'='*80}\")\n",
    "        \n",
    "        print(f\"Loading {training_size:,} samples for training...\")\n",
    "        current_train_datasets = load_training_data_for_stage(\n",
    "            HDF5_TRAIN_FILEPATH, ALL_SNRS, training_size, NUM_SIGNAL_FREQS\n",
    "        )\n",
    "        \n",
    "        if current_train_datasets is None:\n",
    "            print(f\"Could not load data for training size {training_size}. Skipping stage.\")\n",
    "            continue\n",
    "            \n",
    "        print(\"Data loaded. Starting model training...\")\n",
    "        X_train_flat_slice, y_train_flat_slice = current_train_datasets['flat']\n",
    "        X_train_engineered_slice, y_train_engineered_slice = current_train_datasets['engineered']\n",
    "\n",
    "        trained_models_step = {}\n",
    "        \n",
    "        # --- Model Training Phase ---\n",
    "        if 'LGBM' in MODELS_TO_TRAIN_AND_EVAL:\n",
    "            print(\"Training LGBM...\")\n",
    "            model_lgbm = lgb.LGBMClassifier(n_estimators=LGBM_N_ESTIMATORS, random_state=42, verbosity=-1)\n",
    "            model_lgbm.fit(X_train_flat_slice, y_train_flat_slice)\n",
    "            trained_models_step['LGBM'] = model_lgbm\n",
    "\n",
    "        if 'PPO-Engineered' in MODELS_TO_TRAIN_AND_EVAL:\n",
    "            print(\"Training PPO-Engineered with statistical reward...\")\n",
    "            # MODIFICATION: Use the new environment with statistical rewards\n",
    "            env = SignalDetectionEnvWithStats(X_train_engineered_slice, y_train_engineered_slice, window_size=REWARD_WINDOW_SIZE)\n",
    "            total_timesteps = len(X_train_engineered_slice) * RL_TRAINING_EPOCHS\n",
    "            model_ppo = PPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, n_steps=min(2048, len(X_train_engineered_slice)), verbose=0)\n",
    "            model_ppo.learn(total_timesteps=total_timesteps)\n",
    "            trained_models_step['PPO-Engineered'] = model_ppo\n",
    "\n",
    "        if 'MSC' in MODELS_TO_TRAIN_AND_EVAL or 'TFG' in MODELS_TO_TRAIN_AND_EVAL:\n",
    "            print(\"Training Statistical Detectors (ORD)...\")\n",
    "            ord_tester = ORDTester(feature_names=RAW_FEATURE_NAMES)\n",
    "            ord_tester.train(X_train_flat_slice[y_train_flat_slice == 0])\n",
    "            trained_models_step['MSC'] = ord_tester\n",
    "            trained_models_step['TFG'] = ord_tester\n",
    "        \n",
    "        # --- Save models from the final training stage for experimental validation ---\n",
    "        if training_size == CHECKPOINT_TRAINING_SIZE:\n",
    "            print(f\"\\nSaving models trained on {training_size} samples for Phase 2...\")\n",
    "            os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "            for model_name, model in trained_models_step.items():\n",
    "                if model_name == 'PPO-Engineered':\n",
    "                    model.save(os.path.join(MODELS_DIR, f'{model_name}_{training_size}_samples.zip'))\n",
    "                else:\n",
    "                    with open(os.path.join(MODELS_DIR, f'{model_name}_{training_size}_samples.pkl'), 'wb') as f:\n",
    "                        pickle.dump(model, f)\n",
    "            print(\"Models saved.\")\n",
    "\n",
    "        print(f\"\\n--- Starting Evaluation for models trained with {training_size:,} samples ---\")\n",
    "        \n",
    "        # --- Evaluation Phase ---\n",
    "        for eval_trials in tqdm(EVAL_TRIAL_STOPS, desc=\"Evaluation Stages\", leave=False):\n",
    "            for snr in tqdm(ALL_SNRS, desc=f\"Evaluating SNRs (using {eval_trials} trials)\", leave=False):\n",
    "                signal_data_4d, noise_data_4d = load_evaluation_data_for_snr(\n",
    "                    HDF5_EVAL_FILEPATH, snr, eval_trials, NUM_SIGNAL_FREQS\n",
    "                )\n",
    "                \n",
    "                if signal_data_4d.size == 0 or noise_data_4d.size == 0: continue\n",
    "                \n",
    "                num_signal_trials, _, _, n_raw_features = signal_data_4d.shape\n",
    "                num_noise_trials, _, _, _ = noise_data_4d.shape\n",
    "\n",
    "                for model_name, model in trained_models_step.items():\n",
    "                    if model_name == 'PPO-Engineered':\n",
    "                        engineered_signal_4d = generate_features_from_config_vectorized(signal_data_4d, RAW_FEATURE_NAMES, FEATURE_CONFIG)\n",
    "                        engineered_noise_4d = generate_features_from_config_vectorized(noise_data_4d, RAW_FEATURE_NAMES, FEATURE_CONFIG)\n",
    "                        \n",
    "                        _, _, _, n_engineered_features = engineered_signal_4d.shape\n",
    "                        \n",
    "                        features_pd = engineered_signal_4d.reshape(-1, n_engineered_features)\n",
    "                        features_fpr = engineered_noise_4d.reshape(-1, n_engineered_features)\n",
    "                        \n",
    "                        preds_pd = predict_ppo(model, features_pd)\n",
    "                        preds_fpr = predict_ppo(model, features_fpr)\n",
    "                    else:\n",
    "                        features_pd = signal_data_4d.reshape(-1, n_raw_features)\n",
    "                        features_fpr = noise_data_4d.reshape(-1, n_raw_features)\n",
    "                        \n",
    "                        if model_name == 'LGBM':\n",
    "                            preds_pd = predict_lgbm(model, features_pd)\n",
    "                            preds_fpr = predict_lgbm(model, features_fpr)\n",
    "                        elif model_name in ['MSC', 'TFG']:\n",
    "                            preds_pd = predict_statistical(model, features_pd, model_name)\n",
    "                            preds_fpr = predict_statistical(model, features_fpr, model_name)\n",
    "                    \n",
    "                    td_stats = calculate_statistics(preds_pd.reshape(num_signal_trials, -1))\n",
    "                    fpr_stats = calculate_statistics(preds_fpr.reshape(num_noise_trials, -1))\n",
    "\n",
    "                    all_results_over_time.append({'Amostras Treino': training_size, 'Trials Avaliação': eval_trials, 'Modelo': model_name, 'Métrica': 'TD', 'SNR': snr, 'Média': td_stats['mean'], 'Variância': td_stats['variance'], 'P1': td_stats['p1'], 'P99': td_stats['p99']})\n",
    "                    all_results_over_time.append({'Amostras Treino': training_size, 'Trials Avaliação': eval_trials, 'Modelo': model_name, 'Métrica': 'FPR', 'SNR': snr, 'Média': fpr_stats['mean'], 'Variância': fpr_stats['variance'], 'P1': fpr_stats['p1'], 'P99': fpr_stats['p99']})\n",
    "    \n",
    "    print(\"\\n\\n--- SIMULATION EXPERIMENT COMPLETED ---\")\n",
    "    return pd.DataFrame(all_results_over_time)\n",
    "\n",
    "# ##############################################################################\n",
    "# SECTION 5: FINAL PLOTTING AND VISUALIZATION (Unchanged)\n",
    "# ##############################################################################\n",
    "\n",
    "def plot_performance_curves_with_ci(df, training_stop, eval_stop):\n",
    "    \"\"\"Plots TD and FPR vs. SNR curves with 98% confidence intervals.\"\"\"\n",
    "    df_plot = df[(df['Amostras Treino'] == training_stop) & (df['Trials Avaliação'] == eval_stop)]\n",
    "    \n",
    "    if df_plot.empty:\n",
    "        print(f\"\\nWarning: No evaluation data found for {training_stop} training samples and {eval_stop} eval trials.\")\n",
    "        return\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 16), sharex=True)\n",
    "    title = (f'Performance dos Modelos\\n'\n",
    "             f'(Treino: {training_stop:,.0f} amostras, Avaliação: {eval_stop:,.0f} trials)')\n",
    "    fig.suptitle(title, fontsize=18)\n",
    "\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(MODELS_TO_TRAIN_AND_EVAL)))\n",
    "    model_colors = {model: color for model, color in zip(MODELS_TO_TRAIN_AND_EVAL, colors)}\n",
    "\n",
    "    for model in MODELS_TO_TRAIN_AND_EVAL:\n",
    "        model_data = df_plot[(df_plot['Modelo'] == model) & (df_plot['Métrica'] == 'TD')].sort_values('SNR')\n",
    "        if not model_data.empty:\n",
    "            ax1.plot(model_data['SNR'], model_data['Média'], label=f'{model} Média TD', color=model_colors[model], marker='o')\n",
    "            ax1.fill_between(model_data['SNR'], model_data['P1'], model_data['P99'], color=model_colors[model], alpha=0.2)\n",
    "    ax1.set_ylabel('Taxa de Detecção (TD)', fontsize=14)\n",
    "    ax1.set_title('Taxa de Detecção vs. SNR', fontsize=16)\n",
    "    ax1.grid(True, which='both', linestyle=':')\n",
    "    ax1.legend()\n",
    "    ax1.set_ylim(-0.05, 1.05)\n",
    "\n",
    "    for model in MODELS_TO_TRAIN_AND_EVAL:\n",
    "        model_data = df_plot[(df_plot['Modelo'] == model) & (df_plot['Métrica'] == 'FPR')].sort_values('SNR')\n",
    "        if not model_data.empty:\n",
    "            ax2.plot(model_data['SNR'], model_data['Média'], label=f'{model} Média FPR', color=model_colors[model], marker='x')\n",
    "            ax2.fill_between(model_data['SNR'], model_data['P1'], model_data['P99'], color=model_colors[model], alpha=0.2)\n",
    "    ax2.set_xlabel('SNR (dB)', fontsize=14)\n",
    "    ax2.set_ylabel('Taxa de Falsos Positivos (FPR)', fontsize=14)\n",
    "    ax2.set_title('Taxa de Falsos Positivos vs. SNR', fontsize=16)\n",
    "    ax2.grid(True, which='both', linestyle=':')\n",
    "    ax2.legend()\n",
    "    ax2.set_ylim(bottom=-0.01)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "# ##############################################################################\n",
    "# SECTION 6: EXPERIMENTAL VALIDATION (NEW)\n",
    "# ##############################################################################\n",
    "def run_experimental_validation():\n",
    "    \"\"\"\n",
    "    Phase 2: Evaluates models on experimental data using leave-one-out cross\n",
    "    -validation. Compares baseline performance with performance after retraining.\n",
    "    \"\"\"\n",
    "    print(f\"\\n\\n{'='*80}\\n--- STARTING PHASE 2: RETRAINING AND EXPERIMENTAL DATA EVALUATION ---\\n{'='*80}\")\n",
    "    \n",
    "    CHECKPOINT_TRAINING_SIZE = max(TRAINING_SAMPLE_STOPS)\n",
    "\n",
    "    if not os.path.exists(EXPERIMENTAL_FEATURES_FILE):\n",
    "        print(f\"WARNING: Experimental data file '{EXPERIMENTAL_FEATURES_FILE}' not found. Skipping Phase 2.\")\n",
    "        return\n",
    "\n",
    "    with h5py.File(EXPERIMENTAL_FEATURES_FILE, 'r') as f:\n",
    "        volunteers = list(f.keys())\n",
    "    \n",
    "    results = {model: {k:[] for k in ['baseline_tpr','baseline_fpr','retrain_tpr','retrain_fpr']} for model in MODELS_TO_TRAIN_AND_EVAL}\n",
    "\n",
    "    def evaluate_on_experimental(model, X_raw_4d, model_name):\n",
    "        \"\"\"Helper to evaluate any model on the 4D experimental data.\"\"\"\n",
    "        num_trials, _, num_total_freqs, _ = X_raw_4d.shape\n",
    "\n",
    "        if model_name == 'PPO-Engineered':\n",
    "            X_engineered = generate_features_from_config_vectorized(X_raw_4d, RAW_FEATURE_NAMES, FEATURE_CONFIG)\n",
    "            _, _, _, n_features = X_engineered.shape\n",
    "            features_flat = X_engineered.reshape(-1, n_features)\n",
    "            preds = predict_ppo(model, features_flat)\n",
    "        else: # LGBM and Statistical\n",
    "            _, _, _, n_features = X_raw_4d.shape\n",
    "            features_flat = X_raw_4d.reshape(-1, n_features)\n",
    "            if model_name == 'LGBM':\n",
    "                preds = predict_lgbm(model, features_flat)\n",
    "            else: # MSC, TFG\n",
    "                preds = predict_statistical(model, features_flat, model_name)\n",
    "        \n",
    "        preds_reshaped = preds.reshape(num_trials, -1)\n",
    "        # Assumes the first NUM_SIGNAL_FREQS are signal, the rest are noise\n",
    "        tpr = np.mean(preds_reshaped[:, :NUM_SIGNAL_FREQS])\n",
    "        fpr = np.mean(preds_reshaped[:, NUM_SIGNAL_FREQS:])\n",
    "        return tpr, fpr\n",
    "\n",
    "    for tune_volunteer in tqdm(volunteers, desc=\"Cross-Validation Folds\"):\n",
    "        print(f\"\\n--- Fold: Using '{tune_volunteer}' for retraining ---\")\n",
    "        \n",
    "        # Load data for the current fold\n",
    "        with h5py.File(EXPERIMENTAL_FEATURES_FILE, 'r') as f:\n",
    "            X_tune_raw = np.vstack([f[tune_volunteer][intensity]['metrics'][:] for intensity in f[tune_volunteer].keys()])\n",
    "            X_test_list = []\n",
    "            test_volunteers = [v for v in volunteers if v != tune_volunteer]\n",
    "            for v_test in test_volunteers:\n",
    "                for intensity in f[v_test].keys():\n",
    "                    X_test_list.append(f[v_test][intensity]['metrics'][:])\n",
    "        \n",
    "        if not X_test_list:\n",
    "            print(\"  - WARNING: No test data found for this fold. Skipping.\")\n",
    "            continue\n",
    "        X_test_raw = np.vstack(X_test_list)\n",
    "\n",
    "        for model_name in MODELS_TO_TRAIN_AND_EVAL:\n",
    "            # --- 1. Baseline Evaluation ---\n",
    "            model_path_zip = os.path.join(MODELS_DIR, f'{model_name}_{CHECKPOINT_TRAINING_SIZE}_samples.zip')\n",
    "            model_path_pkl = os.path.join(MODELS_DIR, f'{model_name}_{CHECKPOINT_TRAINING_SIZE}_samples.pkl')\n",
    "            \n",
    "            base_model = None\n",
    "            try:\n",
    "                if os.path.exists(model_path_zip): # SB3 models\n",
    "                    base_model = PPO.load(model_path_zip)\n",
    "                elif os.path.exists(model_path_pkl): # Other models\n",
    "                    with open(model_path_pkl, 'rb') as f:\n",
    "                        base_model = pickle.load(f)\n",
    "                else:\n",
    "                    print(f\"  - WARNING: Pre-trained model for {model_name} not found. Skipping.\")\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(f\"  - ERROR loading model {model_name}: {e}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"  Evaluating model: {model_name}\")\n",
    "            tpr_base, fpr_base = evaluate_on_experimental(base_model, X_test_raw, model_name)\n",
    "            results[model_name]['baseline_tpr'].append(tpr_base)\n",
    "            results[model_name]['baseline_fpr'].append(fpr_base)\n",
    "            print(f\"    - Baseline : TPR={100*tpr_base:.3f}%, FPR={100*fpr_base:.3f}%\")\n",
    "\n",
    "            # --- 2. Retraining and Evaluation ---\n",
    "            # Prepare tuning data labels\n",
    "            num_tune_trials, _, num_freqs, _ = X_tune_raw.shape\n",
    "            y_tune_labels_single_trial = np.concatenate([np.ones(NUM_SIGNAL_FREQS), np.zeros(num_freqs - NUM_SIGNAL_FREQS)])\n",
    "            y_tune_flat = np.tile(y_tune_labels_single_trial, num_tune_trials * X_tune_raw.shape[1])\n",
    "            \n",
    "            re_model = None\n",
    "            if model_name == 'PPO-Engineered':\n",
    "                X_tune_engineered = generate_features_from_config_vectorized(X_tune_raw, RAW_FEATURE_NAMES, FEATURE_CONFIG)\n",
    "                _,_,_,n_eng_feats = X_tune_engineered.shape\n",
    "                X_tune_eng_flat = X_tune_engineered.reshape(-1, n_eng_feats)\n",
    "                \n",
    "                env = SignalDetectionEnvWithStats(X_tune_eng_flat, y_tune_flat, window_size=REWARD_WINDOW_SIZE)\n",
    "                re_model = PPO(\"MlpPolicy\", env, n_steps=min(1024, len(X_tune_eng_flat)), verbose=0)\n",
    "                re_model.learn(total_timesteps=len(X_tune_eng_flat)) # Short retraining\n",
    "            \n",
    "            elif model_name == 'LGBM':\n",
    "                _,_,_,n_raw_feats = X_tune_raw.shape\n",
    "                X_tune_flat = X_tune_raw.reshape(-1, n_raw_feats)\n",
    "                re_model = lgb.LGBMClassifier(n_estimators=LGBM_N_ESTIMATORS, random_state=42, verbosity=-1)\n",
    "                re_model.fit(X_tune_flat, y_tune_flat)\n",
    "\n",
    "            elif model_name in ['MSC', 'TFG']:\n",
    "                _,_,_,n_raw_feats = X_tune_raw.shape\n",
    "                X_tune_flat = X_tune_raw.reshape(-1, n_raw_feats)\n",
    "                re_model = ORDTester(feature_names=RAW_FEATURE_NAMES)\n",
    "                re_model.train(X_tune_flat[y_tune_flat == 0])\n",
    "\n",
    "            if re_model:\n",
    "                tpr_re, fpr_re = evaluate_on_experimental(re_model, X_test_raw, model_name)\n",
    "                results[model_name]['retrain_tpr'].append(tpr_re)\n",
    "                results[model_name]['retrain_fpr'].append(fpr_re)\n",
    "                print(f\"    - Retrained: TPR={100*tpr_re:.3f}%, FPR={100*fpr_re:.3f}%\")\n",
    "\n",
    "    # --- Print Final Experimental Results ---\n",
    "    print(\"\\n\\n--- FINAL EXPERIMENTAL RESULTS (MEAN +/- STD DEV) ---\")\n",
    "    for model_name in MODELS_TO_TRAIN_AND_EVAL:\n",
    "        print(f\"\\nResults for: {model_name}\")\n",
    "        for approach in ['baseline', 'retrain']:\n",
    "            tpr_mean = np.mean(results[model_name][f'{approach}_tpr'])\n",
    "            tpr_std = np.std(results[model_name][f'{approach}_tpr'])\n",
    "            fpr_mean = np.mean(results[model_name][f'{approach}_fpr'])\n",
    "            fpr_std = np.std(results[model_name][f'{approach}_fpr'])\n",
    "            if not np.isnan(tpr_mean):\n",
    "                print(f\"  {approach.capitalize():<10}: TPR = {tpr_mean:.3f} ± {tpr_std:.3f} | FPR = {fpr_mean:.3f} ± {fpr_std:.3f}\")\n",
    "\n",
    "# ##############################################################################\n",
    "# SECTION 7: SCRIPT EXECUTION\n",
    "# ##############################################################################\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- PHASE 1: Training on simulated data and plotting performance ---\n",
    "    final_df = run_full_experiment()\n",
    "\n",
    "    if not final_df.empty:\n",
    "        print(\"\\n\\n--- Final Analysis & Visualization ---\")\n",
    "        \n",
    "        snr_for_analysis = -10.0\n",
    "        print(f\"\\n--- Evolution of Mean TD (at SNR={snr_for_analysis}dB) vs. Training Samples ---\")\n",
    "        pivot_td = final_df[\n",
    "            (final_df['Métrica'] == 'TD') & \n",
    "            (final_df['SNR'] == snr_for_analysis)\n",
    "        ].pivot_table(\n",
    "            index='Amostras Treino', columns='Modelo', values='Média'\n",
    "        )\n",
    "        print(pivot_td.to_string())\n",
    "\n",
    "        print(\"\\n\\n--- Generating Performance Plots for Each Training Stage ---\")\n",
    "        for train_stop in TRAINING_SAMPLE_STOPS:\n",
    "            plot_performance_curves_with_ci(final_df, train_stop, max(EVAL_TRIAL_STOPS))\n",
    "    else:\n",
    "        print(\"\\nExperiment (Phase 1) finished, but no results were generated to plot.\")\n",
    "\n",
    "    # --- PHASE 2: Validation on experimental data ---\n",
    "    run_experimental_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11ad978b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "--- Scanning for non-1000 Hz Sampling Frequencies (Fs) ---\n",
      "Scanning directory: C:/Users/alexa/experimental_data/todos/Sinais_EEG/\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking files: 100%|██████████| 67/67 [00:00<00:00, 389.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "--- Scan Complete: Summary ---\n",
      "============================================================\n",
      "\n",
      "❗ Found 26 file(s) with a sampling frequency other than 1000 Hz:\n",
      "\n",
      "  - Subject: 'Ab', Intensity: 'ESP', Fs: 1750.0 Hz\n",
      "  - Subject: 'Bb', Intensity: '30dB', Fs: 1750.0 Hz\n",
      "  - Subject: 'Bb', Intensity: '40dB', Fs: 1750.0 Hz\n",
      "  - Subject: 'Bb', Intensity: '50dB', Fs: 1750.0 Hz\n",
      "  - Subject: 'Bb', Intensity: '60dB', Fs: 1750.0 Hz\n",
      "  - Subject: 'Bb', Intensity: '70dB', Fs: 1750.0 Hz\n",
      "  - Subject: 'Bb', Intensity: 'ESP', Fs: 1750.0 Hz\n",
      "  - Subject: 'Er', Intensity: 'ESP', Fs: 1750.0 Hz\n",
      "  - Subject: 'Lu', Intensity: '30dB', Fs: 1750.0 Hz\n",
      "  - Subject: 'Lu', Intensity: '40dB', Fs: 1750.0 Hz\n",
      "  - Subject: 'Lu', Intensity: '50dB', Fs: 1750.0 Hz\n",
      "  - Subject: 'Lu', Intensity: '60dB', Fs: 1750.0 Hz\n",
      "  - Subject: 'Lu', Intensity: '70dB', Fs: 1750.0 Hz\n",
      "  - Subject: 'Lu', Intensity: 'ESP', Fs: 1750.0 Hz\n",
      "  - Subject: 'So', Intensity: '30dB', Fs: 1750.0 Hz\n",
      "  - Subject: 'So', Intensity: '40dB', Fs: 1750.0 Hz\n",
      "  - Subject: 'So', Intensity: '50dB', Fs: 1750.0 Hz\n",
      "  - Subject: 'So', Intensity: '60dB', Fs: 1750.0 Hz\n",
      "  - Subject: 'So', Intensity: '70dB', Fs: 1750.0 Hz\n",
      "  - Subject: 'So', Intensity: 'ESP', Fs: 1750.0 Hz\n",
      "  - Subject: 'Vi', Intensity: '30dB', Fs: 1750.0 Hz\n",
      "  - Subject: 'Vi', Intensity: '40dB', Fs: 1750.0 Hz\n",
      "  - Subject: 'Vi', Intensity: '50dB', Fs: 1750.0 Hz\n",
      "  - Subject: 'Vi', Intensity: '60dB', Fs: 1750.0 Hz\n",
      "  - Subject: 'Vi', Intensity: '70dB', Fs: 1750.0 Hz\n",
      "  - Subject: 'Vi', Intensity: 'ESP', Fs: 1750.0 Hz\n",
      "\n",
      "Next step: Copy the list of subjects/intensities you want to exclude and send it back to me.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import h5py\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# IMPORTANT: Set this to the directory containing your original .mat files.\n",
    "EXPERIMENTAL_DATA_DIR = 'C:/Users/alexa/experimental_data/todos/Sinais_EEG/'\n",
    "\n",
    "def parse_filename(filepath):\n",
    "    \"\"\"Extracts the volunteer ID and intensity from the filename.\"\"\"\n",
    "    filename = os.path.basename(filepath)\n",
    "    # This regex matches patterns like \"Ab-10dB.mat\" or \"MariaESP.mat\"\n",
    "    match = re.match(r\"([A-Za-z]+)(\\d+dB|ESP)\\.mat\", filename)\n",
    "    if match:\n",
    "        return match.group(1), match.group(2)\n",
    "    return None, None\n",
    "\n",
    "# --- MAIN SCRIPT ---\n",
    "print(\"=\"*60)\n",
    "print(\"--- Scanning for non-1000 Hz Sampling Frequencies (Fs) ---\")\n",
    "print(f\"Scanning directory: {EXPERIMENTAL_DATA_DIR}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find all .mat files in the directory\n",
    "all_mat_files = glob.glob(os.path.join(EXPERIMENTAL_DATA_DIR, '*.mat'))\n",
    "\n",
    "if not all_mat_files:\n",
    "    print(\"\\n❗ No .mat files found in the specified directory. Please check the path.\")\n",
    "else:\n",
    "    problematic_files = []\n",
    "    \n",
    "    # Loop through all found files with a progress bar\n",
    "    for file_path in tqdm(all_mat_files, desc=\"Checking files\"):\n",
    "        volunteer, intensity = parse_filename(file_path)\n",
    "        if not volunteer:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            with h5py.File(file_path, 'r') as f:\n",
    "                # Check if 'Fs' key exists before trying to access it\n",
    "                if 'Fs' not in f:\n",
    "                    print(f\"\\nWARNING: No 'Fs' key in file: {os.path.basename(file_path)}. Skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                fs = f['Fs'][0, 0]\n",
    "                \n",
    "                # The core check\n",
    "                if fs != 1000:\n",
    "                    problematic_files.append({\n",
    "                        \"volunteer\": volunteer,\n",
    "                        \"intensity\": intensity,\n",
    "                        \"fs\": fs\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"\\nERROR: Could not read file {os.path.basename(file_path)}: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"--- Scan Complete: Summary ---\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    if not problematic_files:\n",
    "        print(\"\\n✅ All scanned files have the correct sampling frequency of 1000 Hz.\")\n",
    "    else:\n",
    "        print(f\"\\n❗ Found {len(problematic_files)} file(s) with a sampling frequency other than 1000 Hz:\\n\")\n",
    "        for item in problematic_files:\n",
    "            print(f\"  - Subject: '{item['volunteer']}', Intensity: '{item['intensity']}', Fs: {item['fs']} Hz\")\n",
    "\n",
    "    print(\"\\nNext step: Copy the list of subjects/intensities you want to exclude and send it back to me.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7971a027",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
